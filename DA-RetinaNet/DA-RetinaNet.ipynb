{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Detectron2 Installation <br>\n",
        "1) Select the GPU as hardware in the runtime<br>\n",
        "2) Run the following cell.<br>\n",
        "NB: The runtime will be restarted at the end of this installation which requires few minutes.\n"
      ],
      "metadata": {
        "id": "GXhEjILxLg2f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtYj3_gUlTtV",
        "outputId": "ebc71eaa-13e8-4584-97f4-24afcf9e200f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyyaml==5.1 in /usr/local/lib/python3.10/dist-packages (5.1)\n",
            "Collecting git+https://github.com/facebookresearch/detectron2.git\n",
            "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-915p3rmd\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-915p3rmd\n",
            "  Resolved https://github.com/facebookresearch/detectron2.git to commit 7d2e68dbe452fc422268d40ac185ea2609affca8\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (9.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (3.7.1)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.0.7)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.3.0)\n",
            "Requirement already satisfied: yacs>=0.1.8 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (0.1.8)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (0.9.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.2.1)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (4.66.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.12.3)\n",
            "Requirement already satisfied: fvcore<0.1.6,>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (0.1.5.post20221221)\n",
            "Requirement already satisfied: iopath<0.1.10,>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (0.1.9)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.3.0)\n",
            "Requirement already satisfied: hydra-core>=1.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (1.3.2)\n",
            "Requirement already satisfied: black in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (23.7.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (23.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.23.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (5.1)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from hydra-core>=1.1->detectron2==0.6) (4.9.3)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath<0.1.10,>=0.1.7->detectron2==0.6) (2.7.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (4.42.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (2.8.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (8.1.6)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (1.0.0)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (0.11.2)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (3.10.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (2.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.57.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.4.4)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (2.3.7)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (0.41.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->detectron2==0.6) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->detectron2==0.6) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install pyyaml==5.1\n",
        "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
        "exit(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check the installation\n",
        "The right output running the following cell should be:<br><br>\n",
        "nvcc: NVIDIA (R) Cuda compiler driver<br>\n",
        "Copyright (c) 2005-2020 NVIDIA Corporation<br>\n",
        "Built on Mon_Oct_12_20:09:46_PDT_2020<br>\n",
        "Cuda compilation tools, release 11.1, V11.1.105<br>\n",
        "Build cuda_11.1.TC455_06.29190527_0<br>\n",
        "torch:  1.11 ; cuda:  cu113<br>\n",
        "detectron2: 0.6<br>"
      ],
      "metadata": {
        "id": "9_wvanUINz2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, detectron2\n",
        "!nvcc --version\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "print(\"detectron2:\", detectron2.__version__)"
      ],
      "metadata": {
        "id": "h7aOqIDSlgdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Environment Setup\n",
        "\n",
        "Replace at the following path ../usr/local/lib/python3.7/dist-packages/detectron2/modeling/meta_arch/ the dense_detector.py script with our dense_detector.py.<br>\n",
        "Do the same for the fpn.py file at the path ../usr/local/lib/python3.7/dist-packages/detectron2/modeling/backbone/<br>\n",
        "Load the dataset in Google Drive and import it running the block below."
      ],
      "metadata": {
        "id": "6eWXy_PdO7bT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.config import get_cfg\n",
        "import logging\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "import detectron2.utils.comm as comm\n",
        "from detectron2.checkpoint import DetectionCheckpointer, PeriodicCheckpointer\n",
        "from detectron2.data import MetadataCatalog, build_detection_test_loader, build_detection_train_loader\n",
        "from detectron2.modeling import build_model\n",
        "from detectron2.solver import build_lr_scheduler, build_optimizer\n",
        "from detectron2.utils.events import EventStorage\n",
        "from detectron2.engine import default_argument_parser, default_setup, default_writers, launch\n",
        "import torch, torchvision\n",
        "from detectron2.data.datasets import register_coco_instances, load_coco_json, register_pascal_voc\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "FTL7Qu751ddp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Register your Dataset\n",
        "Run the following cell according to your dataset path. <br>\n",
        "If your annotations are in PASCAL VOC use **register_pascal_voc** otherwise **register_coco_instances** if they are in COCO format.\n",
        "\n"
      ],
      "metadata": {
        "id": "VamGIsg3T5q1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "register_pascal_voc(\"city_trainS\", \"drive/My Drive/cityscape/\", \"train_s\", 2007, ['car','person','rider','truck','bus','train','motorcycle','bicycle'])\n",
        "register_pascal_voc(\"city_trainT\", \"drive/My Drive/cityscape/\", \"train_t\", 2007, ['car','person','rider','truck','bus','train','motorcycle','bicycle'])\n",
        "\n",
        "register_pascal_voc(\"city_testT\", \"drive/My Drive/cityscape/\", \"test_t\", 2007, ['car','person','rider','truck','bus','train','motorcycle','bicycle'])"
      ],
      "metadata": {
        "id": "80n1N7hkQ2xO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This block is an example of how you should use this API\n",
        "register_coco_instances(\"dataset_train_synthetic\", {}, \"drive/My Drive/Bellomo_Dataset_UDA/synthetic/Object_annotations/Training_annotations.json\", \"./drive/My Drive/Bellomo_Dataset_UDA/synthetic/images\")\n",
        "register_coco_instances(\"dataset_train_real\", {}, \"drive/My Drive/Bellomo_Dataset_UDA/real_hololens/training/training_set.json\", \"./drive/My Drive/Bellomo_Dataset_UDA/real_hololens/training\")\n",
        "\n",
        "register_coco_instances(\"dataset_test_real\", {}, \"drive/My Drive/Bellomo_Dataset_UDA/real_hololens/test/test_set.json\", \"./drive/My Drive/Bellomo_Dataset_UDA/real_hololens/test\")"
      ],
      "metadata": {
        "id": "isDeFPTOU9GE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training Loop Definition\n",
        "Run the following block"
      ],
      "metadata": {
        "id": "_1GhaNx9Ur0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logger = logging.getLogger(\"detectron2\")\n",
        "\n",
        "def do_train(cfg_source, cfg_target, model, resume = False):\n",
        "    print(model)\n",
        "    model.train()\n",
        "    optimizer = build_optimizer(cfg_source, model)\n",
        "    scheduler = build_lr_scheduler(cfg_source, optimizer)\n",
        "    checkpointer = DetectionCheckpointer(model, cfg_source.OUTPUT_DIR, optimizer=optimizer, scheduler=scheduler)\n",
        "\n",
        "    start_iter = (checkpointer.resume_or_load(cfg_source.MODEL.WEIGHTS, resume=resume).get(\"iteration\", -1) + 1)\n",
        "    max_iter = cfg_source.SOLVER.MAX_ITER\n",
        "\n",
        "    periodic_checkpointer = PeriodicCheckpointer(checkpointer, cfg_source.SOLVER.CHECKPOINT_PERIOD, max_iter=max_iter)\n",
        "    writers = default_writers(cfg_source.OUTPUT_DIR, max_iter) if comm.is_main_process() else []\n",
        "\n",
        "    i = 1\n",
        "    max_epoch = 41.27 # max iter / min(data_len(data_source, data_target))\n",
        "    current_epoch = 0\n",
        "    data_len = 1502\n",
        "\n",
        "    alpha3 = 0\n",
        "    alpha4 = 0\n",
        "    alpha5 = 0\n",
        "\n",
        "    data_loader_source = build_detection_train_loader(cfg_source)\n",
        "    data_loader_target = build_detection_train_loader(cfg_target)\n",
        "    logger.info(\"Starting training from iteration {}\".format(start_iter))\n",
        "\n",
        "    with EventStorage(start_iter) as storage:\n",
        "        for data_source,data_target, iteration in zip(data_loader_source, data_loader_target, range(start_iter, max_iter)):\n",
        "            storage.iter = iteration\n",
        "\n",
        "            iteration = iteration + 1\n",
        "            if (iteration % data_len) == 0:\n",
        "                current_epoch += 1\n",
        "                i = 1\n",
        "\n",
        "            p = float( i + current_epoch * data_len) / max_epoch / data_len\n",
        "            alpha = 2. / ( 1. + np.exp( -10 * p)) - 1\n",
        "            i += 1\n",
        "\n",
        "            alpha3 = alpha\n",
        "            alpha4 = alpha\n",
        "            alpha5 = alpha\n",
        "\n",
        "            if alpha3 > 0.5:\n",
        "                alpha3 = 0.5\n",
        "\n",
        "            if alpha4 > 0.5:\n",
        "                alpha4 = 0.5\n",
        "\n",
        "            if alpha5 > 0.1:\n",
        "                alpha5 = 0.1\n",
        "\n",
        "            loss_dict = model(data_source, False, alpha3, alpha4, alpha5)\n",
        "            loss_dict_target = model(data_target, True, alpha3, alpha4, alpha5)\n",
        "            loss_dict[\"loss_r3\"] += loss_dict_target[\"loss_r3\"]\n",
        "            loss_dict[\"loss_r4\"] += loss_dict_target[\"loss_r4\"]\n",
        "            loss_dict[\"loss_r5\"] += loss_dict_target[\"loss_r5\"]\n",
        "\n",
        "            loss_dict[\"loss_r3\"] *= 0.5\n",
        "            loss_dict[\"loss_r4\"] *= 0.5\n",
        "            loss_dict[\"loss_r5\"] *= 0.5\n",
        "            losses = sum(loss_dict.values())\n",
        "            assert torch.isfinite(losses).all(), loss_dict\n",
        "\n",
        "            loss_dict_reduced = {k: v.item() for k, v in comm.reduce_dict(loss_dict).items()}\n",
        "            losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
        "            if comm.is_main_process():\n",
        "                storage.put_scalars(total_loss=losses_reduced, **loss_dict_reduced)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            losses.backward()\n",
        "            optimizer.step()\n",
        "            storage.put_scalar(\"lr\", optimizer.param_groups[0][\"lr\"], smoothing_hint=False)\n",
        "            scheduler.step()\n",
        "\n",
        "            if iteration - start_iter > 5 and ((iteration + 1) % 20 == 0 or iteration == max_iter - 1):\n",
        "                for writer in writers:\n",
        "                    writer.write()\n",
        "            periodic_checkpointer.step(iteration)"
      ],
      "metadata": {
        "id": "55_anon91_GB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Configuration Definition\n",
        "Define the configuration for the source (cfg_source) and target dataset (cfg_target). The cfg_source contains also the parameters which will be used by the network such us: learning rate, number of training iterations, weight decay, number of classes etc...\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xdXrpSs-ZlUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg_source = get_cfg()\n",
        "cfg_source.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/retinanet_R_101_FPN_3x.yaml\"))\n",
        "cfg_source.DATASETS.TRAIN = (\"dataset_train_synthetic\",)\n",
        "cfg_source.DATALOADER.NUM_WORKERS = 4\n",
        "cfg_source.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/retinanet_R_101_FPN_3x.yaml\")\n",
        "cfg_source.SOLVER.IMS_PER_BATCH = 4\n",
        "cfg_source.SOLVER.BASE_LR = 0.0002\n",
        "cfg_source.SOLVER.WEIGHT_DECAY = 0.001\n",
        "cfg_source.SOLVER.MAX_ITER = 62000\n",
        "cfg_source.SOLVER.STEPS = (30000,)\n",
        "cfg_source.INPUT.MIN_SIZE_TRAIN = (0,)\n",
        "cfg_source.INPUT.MIN_SIZE_TEST = 0\n",
        "os.makedirs(cfg_source.OUTPUT_DIR, exist_ok=True)\n",
        "cfg_source.MODEL.RETINANET.NUM_CLASSES = 16\n",
        "model = build_model(cfg_source)\n",
        "\n",
        "cfg_target = get_cfg()\n",
        "cfg_target.DATASETS.TRAIN = (\"dataset_train_real\",)\n",
        "cfg_target.INPUT.MIN_SIZE_TRAIN = (0,)\n",
        "cfg_target.DATALOADER.NUM_WORKERS = 2\n",
        "cfg_target.SOLVER.IMS_PER_BATCH = 2"
      ],
      "metadata": {
        "id": "Mr-cPOgv2UXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "do_train(cfg_source,cfg_target,model)"
      ],
      "metadata": {
        "id": "L8lzkX2g2Xc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evalutate the performance\n",
        "runt the COCOEvaluator if your annotations are in COCO otherwhise run the PascalVOCDetectionEvaluator"
      ],
      "metadata": {
        "id": "0M1rLpXVddJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#COCO evaluation example\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "evaluator = COCOEvaluator(\"dataset_test_real\", cfg_source, False, output_dir=\"./output/\")\n",
        "val_loader = build_detection_test_loader(cfg_source, \"dataset_test_real\")\n",
        "inference_on_dataset(model, val_loader, evaluator)"
      ],
      "metadata": {
        "id": "Gu8Jp5IJ2w9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PASCAL VOC evaluation\n",
        "from detectron2.evaluation import inference_on_dataset, PascalVOCDetectionEvaluator\n",
        "from detectron2.data import build_detection_test_loader\n",
        "evaluator = PascalVOCDetectionEvaluator(\"city_testT\")\n",
        "val_loader = build_detection_test_loader(cfg_source, \"city_testT\")\n",
        "res = inference_on_dataset(model, val_loader, evaluator)\n",
        "print(res)"
      ],
      "metadata": {
        "id": "hV27c_2720hk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}