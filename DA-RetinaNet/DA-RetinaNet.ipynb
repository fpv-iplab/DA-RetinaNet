{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DA-RetinaNet.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOv8prw7XF0voQmjzds5xAY"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"IeYPTFrX7BH_"},"source":["#STEP 1\n","run the first two block and restart the runtime\n","\n"]},{"cell_type":"code","metadata":{"id":"XxL9J_SB4n5L"},"source":["!pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n","!pip install pyyaml==5.1 pycocotools>=2.0.1\n","import torch, torchvision\n","print(torch.__version__, torch.cuda.is_available())\n","!gcc --version"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PcKkfmBl4t_1"},"source":["!pip install detectron2==0.2.1 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.6/index.html"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8O7Om0l-7Mts"},"source":["#STEP2 \n","Replace at the following path ```../usr/local/lib/python3.7/dist-packages/detectron2/modeling/meta_arch/``` the retinanet.py script with our retinanet.py. <br>\n","Do the same for the fpn.py file at the path ```../usr/local/lib/python3.7/dist-packages/detectron2/modeling/backbone/```<br>\n","Load the dataset in Google Drive and import it running the block below."]},{"cell_type":"code","metadata":{"id":"McBGQzl24zRA","executionInfo":{"status":"ok","timestamp":1620246506203,"user_tz":-120,"elapsed":1775,"user":{"displayName":"Giovanni Pasqualino","photoUrl":"https://lh6.googleusercontent.com/-w9WGHdtMja0/AAAAAAAAAAI/AAAAAAAAFac/gNqkJw9oZ74/s64/photo.jpg","userId":"18218532099836403290"}}},"source":["import detectron2\n","from detectron2.utils.logger import setup_logger\n","setup_logger()\n","import numpy as np\n","import cv2\n","import random\n","from detectron2 import model_zoo\n","from detectron2.config import get_cfg\n","import logging\n","import os\n","from collections import OrderedDict\n","from torch.nn.parallel import DistributedDataParallel\n","import detectron2.utils.comm as comm\n","from detectron2.checkpoint import DetectionCheckpointer, PeriodicCheckpointer\n","from detectron2.data import MetadataCatalog, build_detection_test_loader, build_detection_train_loader\n","from detectron2.modeling import build_model\n","from detectron2.solver import build_lr_scheduler, build_optimizer\n","from detectron2.utils.events import CommonMetricPrinter, EventStorage, JSONWriter, TensorboardXWriter\n","import torch, torchvision\n","from detectron2.data.datasets import register_coco_instances,load_coco_json\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DnlOi8L988mS"},"source":["#STEP 3\n","Register you dataset using: <br>\n","```register_coco_instances(\"dataset_name_soruce_training\",{},\"path_annotations\",\"path_images\")```<br>\n","```register_coco_instances(\"dataset_name_target_training\",{},\"path_annotations\",\"path_images\")```<br>\n","\n","```register_coco_instances(\"dataset_name_target_test\",{},\"path_annotations\",\"path_images\")```<br>\n","\n"]},{"cell_type":"code","metadata":{"id":"46Qicg3i5GwT"},"source":["register_coco_instances(\"dataset_train_synthetic\", {}, \"drive/My Drive/Bellomo_Dataset_UDA/synthetic/Object_annotations/Training_annotations.json\", \"./drive/My Drive/Bellomo_Dataset_UDA/synthetic/images\")\n","register_coco_instances(\"dataset_train_real\", {}, \"drive/My Drive/Bellomo_Dataset_UDA/real_hololens/training/training_set.json\", \"./drive/My Drive/Bellomo_Dataset_UDA/real_hololens/training\")\n","\n","register_coco_instances(\"dataset_test_real\", {}, \"drive/My Drive/Bellomo_Dataset_UDA/real_hololens/test/test_set.json\", \"./drive/My Drive/Bellomo_Dataset_UDA/real_hololens/test\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pkSaqf_Q9P7x"},"source":["#STEP 4\n","Run the latest block. <br>\n","With large dataset there are some problems with dataloaders. If you encounter these problems, restart the runtime and re-run all the boxes starting from step 2 (included) until it work. \n"]},{"cell_type":"code","metadata":{"id":"DOOfj8tw5QHS"},"source":["logger = logging.getLogger(\"detectron2\")\n","\n","def do_train(cfg_source, cfg_target, model, resume = False):\n","    \n","    model.train()\n","    print(model)\n","    \n","    optimizer = build_optimizer(cfg_source, model)\n","    scheduler = build_lr_scheduler(cfg_source, optimizer)\n","\n","    checkpointer = DetectionCheckpointer(\n","        model, cfg_source.OUTPUT_DIR, optimizer = optimizer, scheduler = scheduler\n","    )\n","    start_iter = (\n","        checkpointer.resume_or_load(cfg_source.MODEL.WEIGHTS, resume = resume).get(\"iteration\", -1) + 1\n","    )\n","    max_iter = cfg_source.SOLVER.MAX_ITER\n","\n","    periodic_checkpointer = PeriodicCheckpointer(\n","        checkpointer, cfg_source.SOLVER.CHECKPOINT_PERIOD, max_iter = max_iter\n","    )\n","\n","    writers = (\n","        [\n","            CommonMetricPrinter(max_iter),\n","            JSONWriter(os.path.join(cfg_source.OUTPUT_DIR, \"metrics.json\")),\n","            TensorboardXWriter(cfg_source.OUTPUT_DIR),\n","        ]\n","        if comm.is_main_process()\n","        else []\n","    )\n","    \n","    i = 1\n","    max_epoch = 41.27 # max iter / min(data_len(data_source, data_target))\n","    current_epoch = 0\n","    data_len = 1502\n","\n","    alpha3 = 0\n","    alpha4 = 0\n","    alpha5 = 0\n","   \n","    data_loader_source = build_detection_train_loader(cfg_source)\n","    data_loader_target = build_detection_train_loader(cfg_target) \n","    logger.info(\"Starting training from iteration {}\".format(start_iter))\n","\n","    with EventStorage(start_iter) as storage:\n","        for data_source,data_target, iteration in zip(data_loader_source, data_loader_target, range(start_iter, max_iter)):\n","            iteration = iteration + 1\n","            storage.step()\n","            \n","            if (iteration % data_len) == 0:\n","                current_epoch += 1\n","                i = 1\n","\n","            p = float( i + current_epoch * data_len) / max_epoch / data_len\n","            alpha = 2. / ( 1. + np.exp( -10 * p)) - 1\n","            i += 1\n","\n","            alpha3 = alpha\n","            alpha4 = alpha\n","            alpha5 = alpha\n","\n","            if alpha3 > 0.5:\n","                alpha3 = 0.5\n","\n","            if alpha4 > 0.5:\n","                alpha4 = 0.5\n","\n","            if alpha5 > 0.1:\n","                alpha5 = 0.1\n","            \n","            loss_dict = model(data_source, False, alpha3, alpha4, alpha5)\n","            loss_dict_target = model(data_target, True, alpha3, alpha4, alpha5)\n","            loss_dict[\"loss_r3\"] += loss_dict_target[\"loss_r3\"]\n","            loss_dict[\"loss_r4\"] += loss_dict_target[\"loss_r4\"]\n","            loss_dict[\"loss_r5\"] += loss_dict_target[\"loss_r5\"]\n","\n","            loss_dict[\"loss_r3\"] *= 0.5\n","            loss_dict[\"loss_r4\"] *= 0.5\n","            loss_dict[\"loss_r5\"] *= 0.5\n","\n","            losses = sum(loss_dict.values())\n","            assert torch.isfinite(losses).all(), loss_dict\n","\n","            loss_dict_reduced = {k: v.item() for k, v in comm.reduce_dict(loss_dict).items()}\n","            losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n","            if comm.is_main_process():\n","                storage.put_scalars(total_loss=losses_reduced, **loss_dict_reduced)\n","\n","            optimizer.zero_grad()\n","            losses.backward()\n","            optimizer.step()\n","            storage.put_scalar(\"lr\", optimizer.param_groups[0][\"lr\"], smoothing_hint=False)\n","            scheduler.step()\n","\n","            if iteration - start_iter > 5 and (iteration % 20 == 0 or iteration == max_iter):\n","                for writer in writers:\n","                    writer.write()\n","            periodic_checkpointer.step(iteration)\n","\n","cfg_source = get_cfg()\n","cfg_source.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/retinanet_R_101_FPN_3x.yaml\"))\n","cfg_source.DATASETS.TRAIN = (\"dataset_train_synthetic\",)\n","cfg_source.DATALOADER.NUM_WORKERS = 4\n","cfg_source.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/retinanet_R_101_FPN_3x.yaml\")\n","cfg_source.SOLVER.IMS_PER_BATCH = 4\n","cfg_source.SOLVER.BASE_LR = 0.0002\n","cfg_source.SOLVER.WEIGHT_DECAY = 0.001\n","cfg_source.SOLVER.MAX_ITER = 62000\n","cfg_source.SOLVER.STEPS = (30000,)\n","cfg_source.INPUT.MIN_SIZE_TRAIN = (0,)\n","cfg_source.INPUT.MIN_SIZE_TEST = 0\n","os.makedirs(cfg_source.OUTPUT_DIR, exist_ok=True)\n","cfg_source.MODEL.RETINANET.NUM_CLASSES = 16\n","model = build_model(cfg_source)\n","\n","cfg_target = get_cfg()\n","cfg_target.DATASETS.TRAIN = (\"dataset_train_real\",)\n","cfg_target.INPUT.MIN_SIZE_TRAIN = (0,)\n","cfg_target.DATALOADER.NUM_WORKERS = 2\n","cfg_target.SOLVER.IMS_PER_BATCH = 2\n","\n","do_train(cfg_source,cfg_target,model)\n","\n","#test\n","from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n","evaluator = COCOEvaluator(\"dataset_test_real\", cfg_source, False, output_dir=\"./output/\")\n","val_loader = build_detection_test_loader(cfg_source, \"dataset_test_real\")\n","inference_on_dataset(model, val_loader, evaluator)"],"execution_count":null,"outputs":[]}]}